# [What is fine-tuning?](#What-is-fine-tuning)


## âœ… What is Fine-Tuning?

**Fine-tuning is the process of taking a pre-trained model and training it further on a smaller, task-specific dataset to adapt it to a particular problem.**

Instead of training from scratch, we reuse the knowledge the model already learned.

---

## ðŸ”¹ Why Fine-Tuning Is Needed?

Training large models from scratch:

* Requires massive data
* Requires huge compute
* Takes weeks or months

So we:

1. Start with a pretrained model (like an LLM trained on internet text)
2. Train it further on domain-specific data

Example:

* Base model â†’ general knowledge
* Fine-tuned model â†’ medical chatbot / coding assistant / CAD assistant

---

## ðŸ”¹ How It Works Internally

1. Load pretrained weights
2. Provide labeled or task-specific data
3. Compute loss on new data
4. Backpropagate gradients
5. Update weights slightly

The model adjusts its parameters to specialize.

---

## ðŸ”¹ Example

Base model:

> Can answer general questions.

After fine-tuning on legal documents:

> Becomes better at legal Q&A.

---

## ðŸ”¹ Types of Fine-Tuning

### 1ï¸âƒ£ Full Fine-Tuning

All model parameters are updated.

### 2ï¸âƒ£ Parameter-Efficient Fine-Tuning (PEFT)

Only small parts of model are trained.
Examples:

* LoRA
* Adapters
* Prefix tuning

This reduces memory and compute cost.

---

## ðŸ”¹ Fine-Tuning vs Prompting

| Fine-Tuning           | Prompt Engineering    |
| --------------------- | --------------------- |
| Changes model weights | No weight changes     |
| Permanent improvement | Temporary instruction |
| Needs training data   | Just better prompts   |

---

## ðŸ”¹ When Do We Use Fine-Tuning?

* Domain adaptation
* Style adaptation
* Instruction following
* Custom behavior
* Improving accuracy on specific tasks

---

## ðŸŽ¯ 30-Second Interview Answer

> Fine-tuning is the process of taking a pretrained model and training it further on a smaller, task-specific dataset to adapt it for a particular use case. Instead of training from scratch, we update the modelâ€™s weights slightly so it becomes specialized while retaining its general knowledge.

---
